{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2 Therapy AI Training on Google Colab\n",
    "\n",
    "This notebook trains a custom Llama 2 model for therapeutic conversations using free Google Colab GPU.\n",
    "\n",
    "## Setup Requirements\n",
    "\n",
    "1. **Google Account** for Colab access\n",
    "2. **Hugging Face Account** with Llama 2 access\n",
    "3. **Training Data** uploaded to Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch accelerate peft bitsandbytes datasets trl\n",
    "!pip install huggingface_hub jsonlines pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Make sure therapy_training_data.jsonl is uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format data for Llama 2 instruction tuning\"\"\"\n",
    "    if example[\"input\"]:\n",
    "        return f\"<s>[INST] {example['instruction']}\\n\\n{example['input']} [/INST] {example['output']}</s>\"\n",
    "    else:\n",
    "        return f\"<s>[INST] {example['instruction']} [/INST] {example['output']}</s>\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = Dataset.from_json(\"therapy_training_data.jsonl\")\n",
    "\n",
    "# Format dataset\n",
    "dataset = dataset.map(lambda x: {\"text\": format_instruction(x)})\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
    "print(\"Sample formatted example:\")\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "model_size = \"7b\"  # @param [\"7b\", \"13b\"]\n",
    "model_name = f\"meta-llama/Llama-2-{model_size}-chat-hf\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation examples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available! Go to Runtime â†’ Change runtime type â†’ T4 GPU\")\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    max_steps=500,  # Limited for Colab free tier\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=250,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"your-username/therapy-llama2-{model_size}-colab\",\n",
    "    hub_token=None,  # Will use logged-in token\n",
    "    report_to=\"none\"  # Disable wandb logging\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Training configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training... This will take 1-2 hours.\")\n",
    "print(\"Monitor the output below for progress.\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Save model locally\n",
    "trainer.save_model(f\"./therapy-llama2-{model_size}\")\n",
    "tokenizer.save_pretrained(f\"./therapy-llama2-{model_size}\")\n",
    "\n",
    "print(f\"Model saved to ./therapy-llama2-{model_size}\")\n",
    "print(\"Model also uploaded to your Hugging Face account!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=f\"./therapy-llama2-{model_size}\",\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"<s>[INST] You are Zensui AI, a compassionate therapy assistant designed to provide supportive, evidence-based therapeutic conversations. You specialize in cognitive behavioral therapy (CBT), trauma-informed care, and mindfulness-based interventions.

IMPORTANT: You are a friendly, helpful AI that can respond to ANY type of question or conversation. While you specialize in therapeutic support, you should:
- Respond warmly and naturally to casual greetings and general questions
- Be conversational and approachable in all interactions
- When users ask non-wellness questions, answer them helpfully while maintaining your caring personality
- Only apply therapeutic techniques when users are discussing emotional, mental health, or wellness topics
- For general questions, be informative and friendly without forcing therapeutic responses

THERAPEUTIC APPROACH:
- Use Cognitive Behavioral Therapy (CBT) techniques to help identify thought patterns
- Apply trauma-informed principles with sensitivity and care
- Incorporate mindfulness and grounding techniques when appropriate
- Use solution-focused brief therapy for practical problem-solving
- Apply dialectical behavior therapy (DBT) skills for emotional regulation

SAFETY PROTOCOLS:
- If someone appears to be in crisis, suicidal, or homicidal, respond with: \"I'm very concerned about your safety and well-being. I'm immediately connecting you with a qualified therapist on our platform who can provide the urgent support you need. You can access our therapists through the 'My Therapist' section of the app, or I can help you book an emergency session right now.\"
- For domestic violence: Provide safety planning and direct to platform therapists
- For substance abuse: Offer harm reduction strategies and direct to platform therapists
- Never provide medical diagnosis or medication advice
- Always maintain professional boundaries

User: Hello, I'm feeling stressed about my studies. Can you help me? [/INST]\"\"\"\n",
    "\n",
    "# Generate response\n",
    "outputs = pipe(\n",
    "    test_prompt,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = outputs[0]['generated_text']\n",
    "# Extract just the AI response (after [/INST])\n",
    "ai_response = response.split(\"[/INST]\")[1].strip()\n",
    "\n",
    "print(\"AI Response:\")\n",
    "print(ai_response)\n",
    "print(\"\\nâœ… Model test successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "!zip -r therapy_llama2_model.zip therapy-llama2-{model_size}/\n",
    "\n",
    "# Download the model\n",
    "from google.colab import files\n",
    "files.download('therapy_llama2_model.zip')\n",
    "\n",
    "print(\"\\nðŸŽ‰ SUCCESS! Your custom Llama 2 therapy AI is ready!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. The model is also uploaded to your Hugging Face account\")\n",
    "print(\"2. Update your backend/.env with the model URL\")\n",
    "print(\"3. Test integration with: node test_llama2_integration.js\")\n",
    "print(\"4. Your therapy platform now has a specialized AI! ðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}