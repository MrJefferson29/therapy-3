{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 2 Therapy AI Training on Google Colab\n",
    "\n",
    "This notebook trains a custom Llama 2 model for therapeutic conversations using free Google Colab GPU.\n",
    "\n",
    "## Setup Requirements\n",
    "\n",
    "1. **Google Account** for Colab access\n",
    "2. **Hugging Face Account** with Llama 2 access\n",
    "3. **Training Data** uploaded to Colab or Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch accelerate peft bitsandbytes datasets trl\n",
    "!pip install huggingface_hub\n",
    "!pip install jsonlines pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional, for saving models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload Training Data\n",
    "\n",
    "Upload your training data files to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# The files should include:\n",
    "# - therapy_training_data.jsonl (your training data)\n",
    "# - intents.json (optional, for reference)\n",
    "# - university_student_knowledge.json (optional, for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Download from your backend\n",
    "# !wget https://your-backend-url/training_data/therapy_training_data.jsonl\n",
    "# !wget https://your-backend-url/training_data/intents.json\n",
    "# !wget https://your-backend-url/training_data/university_student_knowledge.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preparation\n",
    "\n",
    "Load and format the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format data for Llama 2 instruction tuning\"\"\"\n",
    "    if example[\"input\"]:\n",
    "        return f\"<s>[INST] {example['instruction']}\\n\\n{example['input']} [/INST] {example['output']}</s>\"\n",
    "    else:\n",
    "        return f\"<s>[INST] {example['instruction']} [/INST] {example['output']}</s>\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = Dataset.from_json(\"therapy_training_data.jsonl\")\n",
    "\n",
    "# Format dataset\n",
    "dataset = dataset.map(lambda x: {\"text\": format_instruction(x)})\n",
    "\n",
    "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
    "print(\"Sample formatted example:\")\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Setup\n",
    "\n",
    "Load the tokenizer and prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model size\n",
    "model_size = \"7b\"  # @param [\"7b\", \"13b\"]\n",
    "model_name = f\"meta-llama/Llama-2-{model_size}-chat-hf\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation examples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Model with Quantization\n",
    "\n",
    "Load Llama 2 with 4-bit quantization to fit in Colab's GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available!\")\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration\n",
    "\n",
    "Set up the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    max_steps=500,  # Limited for Colab free tier\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=250,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"your-username/therapy-llama2-{model_size}-colab\",\n",
    "    hub_token=None  # Will use logged-in token\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Training\n",
    "\n",
    "Run the training process. This will take 1-2 hours on Colab's free GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save model locally\n",
    "trainer.save_model(f\"./therapy-llama2-{model_size}\")\n",
    "tokenizer.save_pretrained(f\"./therapy-llama2-{model_size}\")\n",
    "\n",
    "print(f\"Model saved to ./therapy-llama2-{model_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test the Model\n",
    "\n",
    "Test your trained model with a sample conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=f\"./therapy-llama2-{model_size}\",\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"<s>[INST] You are Zensui AI, a compassionate therapy assistant designed to provide supportive, evidence-based therapeutic conversations. You specialize in cognitive behavioral therapy (CBT), trauma-informed care, and mindfulness-based interventions.\n",
    "\n",
    "IMPORTANT: You are a friendly, helpful AI that can respond to ANY type of question or conversation. While you specialize in therapeutic support, you should:\n",
    "- Respond warmly and naturally to casual greetings and general questions\n",
    "- Be conversational and approachable in all interactions\n",
    "- When users ask non-wellness questions, answer them helpfully while maintaining your caring personality\n",
    "- Only apply therapeutic techniques when users are discussing emotional, mental health, or wellness topics\n",
    "- For general questions, be informative and friendly without forcing therapeutic responses\n",
    "\n",
    "THERAPEUTIC APPROACH:\n",
    "- Use Cognitive Behavioral Therapy (CBT) techniques to help identify thought patterns\n",
    "- Apply trauma-informed principles with sensitivity and care\n",
    "- Incorporate mindfulness and grounding techniques when appropriate\n",
    "- Use solution-focused brief therapy for practical problem-solving\n",
    "- Apply dialectical behavior therapy (DBT) skills for emotional regulation\n",
    "\n",
    "SAFETY PROTOCOLS:\n",
    "- If someone appears to be in crisis, suicidal, or homicidal, respond with: \"I'm very concerned about your safety and well-being. I'm immediately connecting you with a qualified therapist on our platform who can provide the urgent support you need. You can access our therapists through the 'My Therapist' section of the app, or I can help you book an emergency session right now.\"\n",
    "- For domestic violence: Provide safety planning and direct to platform therapists\n",
    "- For substance abuse: Offer harm reduction strategies and direct to platform therapists\n",
    "- Never provide medical diagnosis or medication advice\n",
    "- Always maintain professional boundaries\n",
    "\n",
    "User: Hello, I'm feeling stressed about my studies. Can you help me? [/INST]\"\"\"\n",
    "\n",
    "# Generate response\n",
    "outputs = pipe(\n",
    "    test_prompt,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = outputs[0]['generated_text']\n",
    "# Extract just the AI response (after [/INST])\n",
    "ai_response = response.split(\"[/INST]\")[1].strip()\n",
    "\n",
    "print(\"AI Response:\")\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download and Deploy\n",
    "\n",
    "Download your trained model and update your backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model for download\n",
    "!zip -r therapy_llama2_model.zip therapy-llama2-{model_size}/\n",
    "\n",
    "# Download the model\n",
    "from google.colab import files\n",
    "files.download('therapy_llama2_model.zip')\n",
    "\n",
    "print(\"\\nModel downloaded! Now:\")\n",
    "print(\"1. Update your backend/.env with the Hugging Face model URL\")\n",
    "print(\"2. Test the integration with: node test_llama2_integration.js\")\n",
    "print(\"3. Your custom Llama 2 therapy AI is ready! 🎉\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}